{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvYPwd4elOUIGUKV6q4vU8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAnandaPhD/RefactorEarth/blob/main/Fine_Tuning_Codebert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning CodeBERT for Code Optimization\n",
        "\n",
        "This notebook provides a step-by-step guide to fine-tuning the CodeBERT model using a custom dataset. CodeBERT is a pre-trained model designed for programming languages, and fine-tuning it on your specific codebase can improve its performance for tasks like code generation, code completion, and other code-related applications.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Setting Up the Environment\n",
        "\n",
        "Let's start by installing the necessary libraries and setting up our environment.\n",
        "\n",
        "`\n"
      ],
      "metadata": {
        "id": "dsHU3P4Rkc9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_02FULIkZx8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll load our dataset. For this example, we'll assume you have a dataset in CSV format with columns input_text (the code) and output_text (the corresponding labels or descriptions)"
      ],
      "metadata": {
        "id": "YMBL6LYNkj3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import git\n",
        "\n",
        "# Specify the repository URL\n",
        "repo_url = \"https://github.com/yourusername/your-repo.git\"\n",
        "\n",
        "# Clone the repository\n",
        "repo_path = \"./your-repo\"\n",
        "git.Repo.clone_from(repo_url, repo_path)\n"
      ],
      "metadata": {
        "id": "bTG319jCkpSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the code files, we'll load and preprocess them for fine-tuning. We'll collect all the Python files (.py files) and prepare them for tokenization"
      ],
      "metadata": {
        "id": "lrN71BIllAZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Function to load Python files from the repository\n",
        "def load_code_files(directory):\n",
        "    code_snippets = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(\".py\"):  # We are focusing on Python files\n",
        "                file_path = os.path.join(root, file)\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    code_snippets.append(f.read())\n",
        "    return code_snippets\n",
        "\n",
        "# Load the code snippets from the cloned repo\n",
        "code_snippets = load_code_files(repo_path)\n",
        "\n",
        "# Display the first few lines of a snippet as an example\n",
        "print(\"\\n\".join(code_snippets[:1][0].splitlines()[:10]))\n"
      ],
      "metadata": {
        "id": "6qUSPycKlD96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll tokenize the code snippets using the CodeBERT tokenizer. Tokenization is necessary to convert the code into a format that the model can understand."
      ],
      "metadata": {
        "id": "W1b6IUMglGPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "# Load the CodeBERT tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "# Tokenize the code snippets\n",
        "def tokenize_snippets(snippets):\n",
        "    return tokenizer(snippets, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_snippets = tokenize_snippets(code_snippets)\n",
        "\n",
        "# Example of a tokenized snippet\n",
        "print(tokenized_snippets['input_ids'][0][:10])  # Show the first 10 tokens of the first snippet\n"
      ],
      "metadata": {
        "id": "HX3DmSFmlJdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we begin fine-tuning, we need to organize the tokenized data into a format suitable for training."
      ],
      "metadata": {
        "id": "u_78qpw2lLv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Convert tokenized snippets into PyTorch tensors\n",
        "input_ids = torch.tensor(tokenized_snippets['input_ids'])\n",
        "attention_masks = torch.tensor(tokenized_snippets['attention_mask'])\n",
        "\n",
        "# Optionally, define labels if you're doing supervised fine-tuning (e.g., code classification)\n",
        "labels = torch.zeros(len(input_ids), dtype=torch.long)  # Example labels\n",
        "\n",
        "# Create a PyTorch dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_sampler = RandomSampler(dataset)\n",
        "train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=8)\n"
      ],
      "metadata": {
        "id": "r8gA_hE9lPdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will fine-tune the CodeBERT model on our dataset."
      ],
      "metadata": {
        "id": "zVJSg8DGlRsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Load the pre-trained CodeBERT model for sequence classification\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./codebert_finetuned\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./codebert_finetuned\")\n"
      ],
      "metadata": {
        "id": "X3vxE5sJlUp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fine-tuning, it's important to evaluate the model to see how well it performs on tasks relevant to your use case."
      ],
      "metadata": {
        "id": "dWFkyvVOlW4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample test code snippets (these would normally come from a separate validation set)\n",
        "test_code_snippets = [\n",
        "    \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\",\n",
        "    \"def add_numbers(a, b):\\n    return a + b\",\n",
        "    \"def fib(n):\\n    if n <= 1:\\n        return n\\n    else:\\n        return fib(n-1) + fib(n-2)\",\n",
        "    \"def print_hello():\\n    print('Hello, world!')\"\n",
        "]\n",
        "\n",
        "# Corresponding labels (1 for recursive, 0 for non-recursive)\n",
        "# The first and third snippets are recursive, so they are labeled with 1.\n",
        "# The second and fourth snippets are non-recursive, so they are labeled with 0.\n",
        "test_labels = [1, 0, 1, 0]\n",
        "\n",
        "# Tokenize the test code snippets\n",
        "test_encodings = tokenizer(test_code_snippets, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "\n",
        "# Convert labels to tensor\n",
        "test_labels_tensor = torch.tensor(test_labels)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "    outputs = model(**test_encodings)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "# Compare the predictions with the actual labels\n",
        "correct_predictions = (predictions == test_labels_tensor).sum().item()\n",
        "total_predictions = len(test_labels)\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Output individual predictions\n",
        "for i, snippet in enumerate(test_code_snippets):\n",
        "    label = \"Recursive\" if predictions[i] == 1 else \"Non-Recursive\"\n",
        "    print(f\"Snippet {i+1}: {label}\")\n"
      ],
      "metadata": {
        "id": "zKpcypbqlatZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you can now use the fine-tuned model for various tasks like code generation, code completion, or any other task that CodeBERT is suitable for"
      ],
      "metadata": {
        "id": "XiMNYsR_lp1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the fine-tuned model\n",
        "from transformers import pipeline\n",
        "\n",
        "code_generator = pipeline(\"text-generation\", model=\"./codebert_finetuned\", tokenizer=tokenizer)\n",
        "generated_code = code_generator(\"def my_function(\", max_length=50, num_return_sequences=1)\n",
        "print(generated_code[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "xWlSSqAclpha"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}